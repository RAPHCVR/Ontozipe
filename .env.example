# ============================
# === FUSEKI (Triple Store) ===
# ============================
# URL complète du dataset Fuseki.
# - "http://fuseki:3030/autonomy" -> quand le backend tourne dans Docker.
# - "http://localhost:3030/autonomy" -> quand le backend tourne sur l'hôte local.
FUSEKI_URL=http://fuseki:3030/autonomy
# Identifiants pour que le backend puisse écrire dans Fuseki (doivent correspondre à shiro.ini)
FUSEKI_USER=admin
FUSEKI_PASSWORD=Pass123

# ============================
# === AUTHENTIFICATION ===
# ============================
# Clé secrète pour la signature des JSON Web Tokens (JWT).
# À changer en production pour une chaîne aléatoire et sécurisée.
JWT_SECRET=changeme

# ============================
# ===     FRONTEND     ===
# ============================
# URL de base du backend, utilisée par le client React (Vite).
VITE_API_BASE_URL=http://localhost:4000/
# URL publique du frontend (utilisée par le backend pour générer des liens vers le graphe).
FRONTEND_BASE_URL=http://localhost:5173/

# ============================
# ===        LLM         ====
# ============================
# Interrupteur global pour tous les usages LLM (assistant, resumes, decisions...)
# Options : 'openai' (defaut) ou 'ollama'
LLM_PROVIDER=openai

# Active des events SSE de debug (non nécessaires pour l’UI standard)
# - tools_schema
# - history_summary
LLM_SSE_DEBUG=0

# TTL du cache de résolution (URIs/predicats) en millisecondes
LLM_RESOLUTION_CACHE_TTL_MS=300000

# --- OPENAI ---
# Cle API et modeles utilises lorsque LLM_PROVIDER=openai
OPENAI_API_KEY=
# Modele principal (assistant, resumes)
OPENAI_MODEL=gpt-5-mini
# Modele decision (taches rapides, classification)
OPENAI_DECISION_MODEL=gpt-4.1-mini

# --- OLLAMA ---
# URL du serveur Ollama (ou passerelle compatible)
OLLAMA_BASE_URL=http://localhost:11434
# Modele par defaut : Magistral (reflexion). Alternative rapide : mistral-small3.2:latest
OLLAMA_MODEL=mistral-small3.2:latest
# Optionnel: jeton si votre passerelle Ollama necessite une Authorization
UTC_API_KEY=
